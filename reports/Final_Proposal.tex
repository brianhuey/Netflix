\documentclass[11pt]{article}
\usepackage{verbatim}
\usepackage[hyphens]{url}
\usepackage{enumerate}
\setlength{\parindent}{0pt}
\setlength{\parskip}{10pt plus 6pt minus 4pt}

\addtolength{\oddsidemargin}{-.5in}
\addtolength{\evensidemargin}{-.5in}
\addtolength{\textwidth}{+1in}


\title{Netflix Final Proposal}
\author{ Brain Huey, Renee Rao}
\date{}


\begin{document}
\maketitle

\begin{abstract}

Instead of working with the 2012 KDD cup dataset on predicting CTRs we
decided to pursue a different question using a different set of
data. We are working the Netflix dataset used in the 2007 KDD cup
competition which provides information on characteristics of users of
the Netflix video services who left ratings for movies from the years
1998 to
2004. (http://www.kdd.org/kdd-cup-2007-consumer-recommendations)

\end{abstract}

\section{Features and Computation}

We will continue to engineer a feature created with our non-Negative
Matrix Factorization (NNMF) algorithm on the whole dataset. The idea
behind using NNMF to predict user ratings behavior is that there
should be some latent factors in the data that determine how a user
rates an item. For example, two users might both rate the same
romantic comedy because they both tend to enjoy watching movies of
that genre. After a latent factor is uncovered it can be used to
predict rating behavior of a user-movie pair because the features
associated with the user should match the features associated with a
movie. The neat thing about NNMF is the algorithm works to fill in the
``unknown'' entries of our sparse movie-user matrix in a way that is
consistent with the ``known'' entries (i.e. ratings so far) by
approximately decomposing the movie-user matrix into the product of
two matrices $A = U x V$ using our gradient descent based
algorithm. Assuming we wish to find k latent factors; then each row of
$V$ (of length $k$ represents the strength of the associations between
a movie and the latent factors. Similarly, each column of V ($k$, \# of
users) would represent the strength of the associations between the
user and the factors.  

The product of these two matrices is viewed as an approximation the
original data by linear combinations of the latent factors. So in this
sense the NNMF model engineers feature vectors without specification
of what the latent k factors actually are! For the final project after
converting the current python script of NNMF to run in MapReduce we
will experiment with how our model predictions change with changes in
the number of latent factors we ask the model to account for. In our
preliminary experiments, we have set the number of latent factors to
be 5 and seem to already get an interesting feature. The code for this
is already written to work as a python script. We will have to run
this on the full set of data. For the final project the python script
should be converted in a mapReduce job.  As mentioned in our progress
report we will also try to improve this model by implementing baseline
probability factors for each movie-user pair based on a set of
features we have engineered using information from the netflix dataset
and and the website Rotten Tomatoes about movie release dates, the
first time a user has rated something, as well average probabilities
for the user and movie in any user-movie pair. We already have code to
compute these features using MapReduce, so for the final project we
need only run the MapReduce jobs on the whole data set and suitably
integrate the resulting features into the MapReduce version of our
NNMF model.

\section{Assessment}

We will continue to use the RMSE between our training and validation
sets (spilt on movie ratings before and after 2005) to assess our
models (thus saving any data for 2006 for our final test set). During
the course of the project we have also found it help to assess
learning in the NNMF model by looking at the distribution of the the
NNMF feature for movie-user pairs that were actually rated versus
random movie-user-pairs. We have also been writing specific test cases
for our models simply to assess if they work they way that we intend
them to work and will continue to do so.


\section{Models and methods: improvements and application}

Currently the method used to ensure all entries of the component
matrices are not negative yields very non-sparse vectors, since it
makes all numbers be strictly greater than zero. We would like to
explore whether setting small numbers to zero in the final prediction
phase will improve generalization.  Explicitly driving small numbers
down and big numbers up in the factors should make them more
"sparse". For our final project we might like to add this to the
gradient descent routine. We will also integrate our baseline
probabilities from the features described above into the NNMF model to
improve it for our final project.  We view the NNMF model to be output
a feature for each user-movie pair.

We will use this feature along with baseline to train other learning
methods primarily a logistic regression model since we wish to output
probabilities. It may also be interesting to try to build a K Nearest
Neighbors (KNN) model for the data to compare results to our original
NNMF model since the two methods are theoretically equivalent. So far
we have used the packages numpy, math, random, scipy.sparse and we
will likely use the package sklearn to do logistic regression.

\section{Division of Labor}

We will continue to split the tasks evenly between ourselves: we plan
to finish up the last bit of modeling and then begin refining our
project and writing up the report over the two weeks.

\section{Foreseeable Difficulties}

The final putting together of the features into a learning
algorithm is likely to be challenging.  In particular, 
from the contest winners writeups, it is clear that
small shifts in even one parameter (such as background
probability) shifts the final scores substantially.
We anticipate that this task may be quite challenging.

Still, we are hopeful, as our feature does seem
to distinguish ratings from non-ratings pretty well,
so applying logistic regression on it may work
well.  We have not had time to address this
as working with the data, building baselines, building and testing
our NNMF algorithm, and evaluating these and report
preparation has been time consuming. 

Of course, the final presentations both written and oral
will provide some challenge as well.


\begin{thebibliography}{}

\bibitem{first}
Mikl{\'o}s Kurucz, Andr{\'a}s~A Bencz{\'u}r, Tam{\'a}s Kiss, Istv{\'a}n Nagy,
  Adrienn Szab{\'o}, and Bal{\'a}zs Torma.
\newblock Kdd cup 2007 task 1 winner report.
\newblock {\em SIGKDD Explorations}, 9(2):53--56, 2007.

\bibitem{miklos}
"Matrix Factorization: A Simple Tutorial and Implementation in Python." Quuxlabs. N.p., n.d. Web. 17 Nov. 2014.

\bibitem{metrics}
"KDD Cup 2007: Consumer Recommendations." Sig KDD. N.p., n.d. Web. 17 Nov. 2014.

\bibitem{wiki}
C. Ding, X. He, H.D. Simon (2005). "On the Equivalence of Nonnegative Matrix Factorization and Spectral Clustering". Proc. SIAM Int'l Conf. Data Mining, pp. 606-610. May 2005



\end{thebibliography}
  

\end{document}
